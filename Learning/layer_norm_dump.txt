    // Note: It works like crap.
    inl layer_norm =
        inl fwd o i s =
            inl o_primal = s.CudaTensor.to_dev_tensor o.primal
            inl n = (primal i).dim |> snd |> HostTensor.span |> to float
            s.CudaKernel.map_d1_seq_broadcast {
                seq = 
                    {
                    redo=(+)
                    map_out=inl i sum -> i - sum / n
                    }
                    ,
                    {
                    map_in=inl v -> v*v
                    redo=(+)
                    map_out=inl v vv -> 
                        inl o = o_primal 0 .get
                        v / sqrt (o*o + vv / n)
                    }
                } (primal i)

        inl bck o r i s =
            inl o = Struct.map s.CudaTensor.to_dev_tensor {o without block}
            inl n = (primal i).dim |> snd |> HostTensor.span |> to float
            s.CudaKernel.map_d1_seq_broadcast' {
                seq = 
                    {
                    map_in=inl dr,i -> i
                    redo=(+)
                    map_out=inl dr,i sum -> dr, i - sum / n
                    }
                    ,
                    {
                    map_in=inl dr,v -> v*v
                    redo=(+)
                    map_out=inl dr,v vv -> 
                        inl o = o .primal 0 .get
                        dr,v,sqrt (o*o + vv / n)
                    }
                    ,
                    {
                    map_in=inl dr,v,norm -> -dr * v / (norm * norm)
                    redo=(+)
                    map_out=inl dr,v,norm dnorm -> 
                        inl dv_top = dr / norm
                        inl dv_norm = dnorm / (two * norm)
                        dv_top,v,dv_norm
                    }
                    ,
                    {
                    map_in=inl _,_,dv_norm -> dv_norm
                    // redo' does not do broadcasting to the zeroth thread.
                    redo'=(+)
                    map_out=inl dv_top,v,dv_norm sum_dv_norm -> 
                        if threadIdx.x = 0 then two * o.primal 0 .get * sum_dv_norm |> atomic_add (o.adjoint 0)
                        inl dv_bot = dv_norm * (two / n) * v 
                        dv_top + dv_bot
                    }
                    ,
                    {
                    redo=(+)
                    map_out=inl dv dv_mean adjoint -> adjoint + dv - dv_mean / n
                    }
                } (adjoint r, primal i) (adjoint i)

        inl init s = s.CudaTensor.zero {elem_type=float; dim=1} |> dr s

        inl activation o i s =
            inl r = fwd o i s |> dr s
            r, inl _ -> bck o r i s

        {fwd bck init activation} |> stackify

    // The feedforward layer with layer norm.
    inl layer_ln initializer activation size sublayer =
        feedforward
            {
            size sublayer
            weights = inl s -> {
                input = initializer (sublayer.size, size) s |> dr s
                bias = s.CudaTensor.zero {elem_type=float; dim=size} |> dr s
                o = layer_norm.init s
                ln = initializer (size, size) s |> dr s
                bias_ln = s.CudaTensor.zero {elem_type=float; dim=size} |> dr s
                }
            apply = inl weights input -> 
                matmultb (input, weights.input) weights.bias 
                >>= layer_norm.activation weights.o 
                >>= inl ln -> matmultb (ln, weights.ln) weights.bias_ln
                >>= activation
            }

    inl sigmoid_ln = layer_ln Initializer.sigmoid Activation.sigmoid

    /// The multiplicative integration RNN with layer norm from the 'Normalizing the Normalizers' paper.
    inl miln size sublayer = 
        recurrent 
            {
            size sublayer
            weights = inl s ->
                open Initializer
                inl bias0 _ = s.CudaTensor.zero {elem_type=float; dim=size} |> dr s
                inl bias init = 
                    inl x = s.CudaTensor.create {elem_type=float; dim=size} 
                    join s.CudaTensor.mmap (const (dyn init)) x
                    dr s x
                {
                input = sigmoid (sublayer.size, size) s |> dr s
                state = sigmoid (size, size) s |> dr s
                b1 = bias one
                b2 = bias (to float 0.5)
                b3 = bias (to float 0.5)
                b4 = bias0 ()
                o = layer_norm.init s
                } |> heap

            apply = inl {b1 b2 b3 b4 input state o} s i ->
                match s with
                | () ->
                    inm i = matmult (i, input)
                    d2_replicate_activation {
                        fwd=inl (b3,b4) i -> b3*i + b4 
                        bck_in=inl (b3,b4) i out -> (i, one) 
                        bck_in'=inl (b3,b4) i out -> b3 
                        } (b3,b4) i
                | _ ->
                    inm i = matmult (i, input)
                    inm s = matmult (s, state)
                    d2_replicate_activation {
                        fwd=inl (b1,b2,b3,b4) (i,s) -> b1*i*s + b2*s + b3*i + b4
                        bck_in=inl (b1,b2,b3,b4) (i,s) out -> (i*s, s, i, one) 
                        bck_in'=inl (b1,b2,b3,b4) (i,s) out -> (b1*s+b3, b1*i+b2)
                        } (b1,b2,b3,b4) (i,s)
                >>= layer_norm.activation o
                >>= Activation.sigmoid
                >>= inl x -> succ (x,x)
            }

