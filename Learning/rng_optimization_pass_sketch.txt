                inl o = weights.o
                inl v = weights.v
                inl batch_num, z_num = z.dim
                inl z_num v_num, x_num = v.dim

                inl size = to float size

                // c1 = - size * log (sqr o)
                inl c1 = s.CudaKernel.map (inl o -> -size * two / o) o

                // c2 = - (log << det) (I + 1 / o^2 * V^T V)
                //z = batch_num x z_num
                //v = z_num x v_num x x_num
                //x = batch_num x x_num
                // TODO: This is broken. There is a step missing in the backward pass, namely multiplying by i_oo_vtv.
                inl c2 =
                    // V^T V
                    // Adjusted as Spiral is in row major format.
                    // vtv = z_num x v_num x v_num
                    inl vtv = s.CudaBlas.gemm_strided_batched .nT .T 1f32 v v
                    // I + 1 / o^2 * V^T V
                    // i_oo_vtv = z_num x v_num x v_num
                    inl i_oo_vtv = 
                        inl o,vtv = Tuple.map CudaAux.to_dev_tensor (o,vtv)
                        s.CudaKernel.init {dim=vtv.dim} (
                            inl i ->
                                inl o = o i .get
                                inl vtv = vtv i
                                inl j k ->
                                    inl I = if j = k then one else zero
                                    I + 1 / (o*o) * vtv j k .get
                            )

                    // (I + 1 / o^2 * V^T V)^-1
                    // i_oo_vtv_inv = z_num x v_num x v_num
                    // Is the error when at the top which is the case here.
                    inl i_oo_vtv_inv = s.CudaBlas.matinv_batched_asserted i_oo_vtv
                    
                    // d -(log << det) (I + 1 / o^2 * V^T V) / d o
                    inl o_bck = 
                        // d o^-2 * vtv / d o = -2 * o^-3 * vtv
                        // o_bck_inner = z_num x v_num x v_num
                        inl o_bck_inner = 
                            inl o,vtv = Tuple.map CudaAux.to_dev_tensor (o,vtv)
                            s.CudaKernel.init {dim=vtv.dim} (
                                inl i ->
                                    inl o = o i .get
                                    inl vtv = vtv i

                                    // Note that left should be -two / (o*o*o)
                                    // The result is rescaled by the negative sign at the top.
                                    inl left = two / (o*o*o)
                                    inl j k -> left * vtv j k .get
                                )

                        // i_oo_vtv_inv = z_num x v_num x v_num
                        // o_bck_inner = z_num x v_num x v_num
                        s.CudaBlas.gemm_strided_batched .nT T one i_oo_vtv_inv o_bck_inner
                        |> inl x -> x.reshape (inl a,b,c -> a,b*c)
                        |> inl x -> s.CudaKernel.mapi_d1_redo_map {neutral_elem=zero; redo=(+)} x ()

                    // d -(log << det) (I + 1 / o^2 * V^T V) / d V
                    inl v_bck =
                        // rescaled_i_oo_vtv_inv = z_num x v_num x v_num
                        inl rescaled_i_oo_vtv_inv = 
                            inl o,i_oo_vtv_inv = Tuple.map CudaAux.to_dev_tensor (o,i_oo_vtv_inv)
                            s.CudaKernel.init {dim=i_oo_vtv_inv.dim} (
                                inl i ->
                                    inl o = o i .get
                                    inl i_oo_vtv_inv = i_oo_vtv_inv i

                                    inl j k -> 
                                        // The rescaling by the derivative should be
                                        // one / (o*o) * i_oo_vtv_inv j k .get
                                        // The rescaling by the negative sign at the top and the adjustment 
                                        // for the following matrix multiplication times the derivative is added to the formula.
                                        -two / (o*o) * i_oo_vtv_inv j k .get
                                )
                            
                        /// TODO: Reverse all the transposes when done. Spiral does it in row major order.
                        s.CudaBlas.gemm_strided_batched .nT T one rescaled_i_oo_vtv_inv v

                    o_bck, v_bck
                
                // c3 = o^2 * reduce_mean (z * dot x x)
                //x = batch_num x x_num
                //z = batch_num x z_num
                // TODO: This is broken. mapi_d2_redo_map does not have mapi_out.
                inl c3 = 
                    s.CudaKernel.mapi_d1_redo_map {
                        map_in=inl x -> x*x
                        neutral_elem=zero; redo=(+)
                        } x ()
                    |> inl x ->
                        inl x,o = Tuple.map CudaAux.to_dev_tensor (x,o)
                        s.CudaKernel.mapi_d2_redo_map {
                            mapi_in=inl i -> // TODO: The order of dimensions is probably wrong here.
                                inl x = x i .get
                                inl _ z -> x * z
                            neutral_elem=zero; redo=(+)
                            mapi_out=inl i ->
                                inl o = o i .get * two
                                inl _ r -> o * r
                            } z ()

                // c4 = sum (inl i -> reduce_mean (z * sqr (dot (v i) x)))
                //```
                //z = batch_num x z_num
                //v = z_num x v_num x x_num
                //x = batch_num x x_num
                //```
                //The double sum can be simplified into a series of matrix multiplications. The imporant thing is to keep track of dimensions.
                //```
                //xv = batch_num x z_num x v_num
                //```
                //The first operation is `x * transpose v`. Since `v` is 3d, the first two dimensions of it are flattened
                //before being passed into the gemm kernel.
                //```
                //z_rep = batch_num x z_num x v_num
                //```
                //Since each `z` element touches each of the `xv`s, it is replicated `v_num` times so the dimensions match up.
                //In actual code, rather than do that the init kernel is used instead to do that implicitly.
                //`z_rep` is then used in an Hadamard multiplication with `xv`.
                //```
                //zxv = batch_num x z_num x v_num
                //```
                //The derivative of the fourth term requires a multiplication by another x.
                //```
                //zxv_rep = batch_num x z_num x v_num x x_num
                //```
                //In the code, the replication is done implicitly inside the kernel.
                //```
                //zxvx = batch_num x z_num x v_num x x_num
                //```
                //The finally to get the derivative with respect to v, the batch dimension needs to be reduced.
                //```
                //zxvx' = z_num x v_num x x_num
                //```
                inl c4 = // TODO: This needs a more efficient way of doing reduction.
                    // d (sqr (x v)) / d v = two * dot (S s) x * x
                    inl xv =
                        inl v = v.reshape (inl z_num,v_num,x_num -> z_num * v_num, x_num)
                        s.CudaBlas.gemm .nT T one x v
                            .reshape (inl batch_num, zv_num -> batch_num, v_num, zv_num / v_num)
                    // d (sqr (x v)) / d v = two * dot (S s) x * x
                    inl zxv = 
                        inl z,xv = Tuple.map CudaAux.to_dev_tensor (z,xv)
                        s.CudaKernel.init {dim=batch_num,z_num,v_num} (inl b ->
                            inl z = z b
                            inl xv = xv b
                            inl z' ->
                                inl z = z z' .get
                                inl xv = xv z'
                                inl v' ->
                                    inl xv = xv v' .get
                                    /// The rescaling for the mean of the batch size and derivative of sqr is done here.
                                    two / to float batch_num * z * xv 
                            )
                    inl zxvx =
                        inl zxv, x = Tuple.map CudaAux.to_dev_tensor (zxv,x)
                        s.CudaKernel.init {dim=batch_num, z_num, v_num, x_num} (
                            inl b ->
                                inl zxv = zxv b
                                inl x = x b
                                inl z ->
                                    inl zxv = zxv z
                                    inl v ->
                                        inl zxv = zxv v .get
                                        inl x' ->
                                            inl x = x x' .get
                                            zxv * x
                            )
                        |> inl x -> s.CudaKernel.mapi_d2_redo_map {neutral_elem=zero; redo=(+)} x ()
                    zxvx

                //inl o_bck = c1 + fst c2 + c3
                //inl v_bck = snd c2 + c4

                // - size * log (sqr o) - (log << det) (I + 1 / o * dot V V) + o^2 * reduce_mean (z * dot x x) +
                // sum {from=1; near_to=S.dim_outer} (inl s -> reduce_mean (z * sqr (dot (S s) x))) + C1
