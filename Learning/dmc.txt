    /// The differentiable action selectors.
    inl Selector =
        inl reduce_actions_template map_out x s = 
            s.CudaKernel.mapi_d1_redo_map {
                mapi_in=inl j i v _ -> v, i
                neutral_elem=-infinity,-1
                redo=inl a b -> if fst a > fst b then a else b
                map_out
                } x ()

        inl reduce_actions = reduce_actions_template snd
        inl reduce_actions' = reduce_actions_template id

        inl pg selector x s =
            inl dim_a, dim_b = primal x .dim
            inl batch_size = HostTensor.span dim_a
            assert (batch_size = 1) "Only a single dimension for now."

            inl p = softmax one (primal x) s
            inl a = selector p s

            a, inl (reward: float64) ->
                inl batch_size = to float batch_size
                inl reward = to float reward
                inl x_a = to_dev_tensor (adjoint x)
                inl p = to_dev_tensor p
                inl a = to_dev_tensor a
                s.CudaKernel.iter () (inl j ->
                    inl x_a = x_a j
                    inl p = p j
                    inl a = a j .get

                    inl i ->
                        inl p = p i .get
                        inl x_a = x_a i
                        inl label = if a = i then one else zero
                        x_a.set (x_a.get + (p - label) * reward / batch_size) 
                    ) (dim_a, dim_b)

        {
        greedy_pg = pg reduce_actions
        sampling_pg = pg sample_body

        greedy_square = inl x s ->
            inl dim_a, dim_b = primal x .dim
            inl batch_size = HostTensor.span dim_a
            assert (batch_size = 1) "Only a single dimension for now."

            inl v,a = reduce_actions' x s |> HostTensor.unzip
            a,inl (reward: float64) ->
                inl batch_size = to float batch_size
                inl reward = to float reward
                inl v,a,x = Tuple.map to_dev_tensor (v,a,adjoint x)
                s.CudaKernel.iter () (inl j ->
                    inl a, v = Tuple.map (inl x -> x j .get) (a, v)
                    inl x = x j a
                    x.set (x.get + square_bck (v, reward) / batch_size)
                    ) dim_a

        sampling_square = inl temp x s ->
            inl dim_a, dim_b = primal x .dim
            inl batch_size = HostTensor.span dim_a
            assert (batch_size = 1) "Only a single dimension for now."

            inl p = softmax temp (primal x) s
            inl a = sample_body p s

            a,inl (reward: float64) ->
                inl batch_size = to float batch_size
                inl reward = to float reward
                inl a,x_p,x_a = Tuple.map to_dev_tensor (a,primal x,adjoint x)
                s.CudaKernel.iter () (inl j ->
                    inl a = a j .get
                    inl x_p,x_a = Tuple.map (inl x -> x j a) (x_p,x_a)
                    inl v = x_p.get
                    x_a.set (x_a.get + square_bck (v, reward) / batch_size)
                    ) dim_a
        }

    inl RL =
        inl greedy_layer apply sublayer =
            Layer.layer {
                layer_type = .action_ff
                size = 1
                sublayer
                apply
                }

        inl greedy_init {range state_type action_type} s =
            inl size = Struct.foldl (inl s x -> s + SerializerOneHot.span range x) 0
            inl state_size = size state_type
            inl action_size = size action_type

            input .input state_size
            //|> Feedforward.Layer.ln 0f32 256
            //|> Feedforward.Layer.tanh 256
            //|> Recurrent.Layer.mi 256
            //|> Recurrent.Layer.tanh 256
            //|> Recurrent.Layer.mi 256
            //|> Recurrent.Layer.mi action_size
            //|> Feedforward.Layer.ln 0f32 action_size
            |> Feedforward.Layer.linear action_size
            |> init s

        /// For online learning.
        inl action {range state_type action_type net state} i s =
            indiv join
                assert (eq_type state_type i) "The input must be equal to the state type."
                inl one_hot_tensor l, size = s.CudaKernel.init {dim=1,size} (inl _ x -> Struct.foldl (inl s x' -> if x = x' then one else s) zero l)
                inl input = 
                    Struct.foldl_map (inl s x -> 
                        inl i, s' = SerializerOneHot.encode' range x
                        s + i, s + s'
                        ) 0 i
                    |> one_hot_tensor

                inl a, {state bck} = run net {state input={input}; bck=const()} s
                inl action = SerializerOneHot.decode range (s.CudaTensor.get (a 0)) action_type
                stack (action, {bck state})

        {greedy_init greedy_layer Selector action}


