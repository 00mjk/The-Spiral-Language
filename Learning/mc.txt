    /// The differentiable action selectors.
    inl Selector =
        inl reduce_actions x s = 
            s.CudaKernel.mapi_d1_redo_map {
                mapi_in=inl j i v _ -> v, i
                neutral_elem=-infinity,-1
                redo=inl a b -> if fst a > fst b then a else b
                } (primal x) ()
            |> HostTensor.unzip

        inl greedy_square_bck_template x v a s = inl (reward: float64) ->
                inl reward = to float reward
                s.CudaKernel.iter () (inl j ->
                    inl a, v = Tuple.map (inl x -> x j) (a, v)
                    inl x = x j a
                    x.set (x.get + square_bck (v, reward))
                    ) (fst x.dim)

        inl greedy_square_bck x v a s = inl (reward: float64) ->
            inl a, x, v = Tuple.map s.CudaTensor.to_dev_tensor (a, adjoint x, v)
            inl f x j = x j .get
            greedy_square_bck_template x (f v) (f a) s reward

        inl greedy_square_bck' x v (a: int64) s = inl (reward: float64) ->
            inl x, v = Tuple.map s.CudaTensor.to_dev_tensor (adjoint x, v)
            greedy_square_bck_template x (inl j -> v j .get) (const a) s reward

        {
        greedy_square = inl x s ->
            inl v,a = reduce_actions x s
            a, greedy_square_bck x v a s

        greedy_square' = inl (a: int64) x s ->
            assert ((primal x).span_outer = 1) "This is for online learning only."
            inl v,_ = reduce_actions x s
            a, greedy_square_bck' x v a s

        greedy_qr = inl k x s ->
            inl dim_a,dim_b,dim_c as dim = (primal x).dim
            inl dim_c' = to float (HostTensor.span dim_c)
            inl v,a =
                s.CudaKernel.mapi_d1_dredo_map { 
                    redo_in = {
                        neutral_elem=0f32
                        redo=(+)
                        }
                    redo_mid = {
                        mapi_in=inl j i a -> a, i
                        neutral_elem=-infinityf32,-1
                        redo=inl a b -> if fst a > fst b then a else b
                        }
                    map_out = inl a, i -> a / dim_c', i
                    } (primal x)
                |> HostTensor.unzip

            a, inl (reward: float64) ->
                inl reward = to float reward
                inl a, x_a, x_p = Tuple.map s.CudaTensor.to_dev_tensor (a, adjoint x, primal x)

                s.CudaKernel.iter () (inl j ->
                    inl a = a j .get
                    inl x_a, x_p = Tuple.map (inl x -> x j a) (x_a, x_p)
                    inl i ->
                        inl x_a, x_p = Tuple.map (inl x -> x i) (x_a, x_p)
                        inl quantile = (to float i - to float 0.5) / dim_c'
                        x_a.set (x_a.get + HQR.bck_a k quantile (x_p.get, reward))
                    ) (dim_a,dim_c)

        greedy_kl = inl x s ->
            inl dim_a,dim_b,dim_c as dim = (primal x).dim

            inl index c x next = 
                inl f i = Struct.map ((|>) i)
                inl rec loop c x =
                    if c > 0 then inl i -> loop (c-1) (f i x)
                    else next x
                assert (lit_is c && c >= 0) "c must be a literal greater or equal to zero."
                loop c x

            inl v,a =
                inl o = s.CudaTensor.create {elem_type=float; dim=dim_a, dim_b}
                inl _ = 
                    inl x,o = Tuple.map s.CudaTensor.to_dev_tensor (primal x, o)
                    s.CudaKernel.iteri_dd1_seq_broadcast { 
                        mapi_in =
                            inb x = index 3 x
                            x.get
                        seq = 
                            {
                            redo=max
                            map_out=inl x max_x -> exp (x - max_x) 
                            }
                            ,
                            {
                            redo=(+)
                            mapi_out=inl k j i z sum_z -> 
                                inl z = z / sum_z
                                z * to float i
                            }
                            ,
                            {
                            redo'=(+)
                            mapi_out=
                                inb o = index 2 o
                                inl _ _ sum -> if threadIdx.x = 0 then o.set sum
                            }
                        } dim
                reduce_actions o s            
            //Console.printfn "{0}, {1}" (s.CudaTensor.get (v 0), s.CudaTensor.get (a 0))

            a, inl (reward: float64) ->
                inl reward = to int64 reward

                assert (dim_c.from <= reward && reward < dim_c.near_to) "The reward must be in range."
                inl a, x_p, x_a = Tuple.map s.CudaTensor.to_dev_tensor (a, primal x, adjoint x)
                s.CudaKernel.iteri_d1_seq_broadcast { 
                    mapi_in = inl j ->
                        inl a = a j .get
                        inl x_p = x_p j a
                        inl i -> a, x_p i .get
                    seq = 
                        {
                        map_in=snd
                        redo=max
                        map_out=inl (a,x) max_x -> a, exp (x - max_x) 
                        }
                        ,
                        {
                        map_in=snd
                        redo=(+)
                        mapi_out=inl j ->
                            inl x_a = x_a j
                            inl i (a,z) sum_z -> 
                                inl p = z / sum_z
                                inl reward = if i = reward then one else zero
                                inl o = p - reward
                                inl x_a = x_a a i
                                x_a .set (x_a .get + o)
                        }
                    } (dim_a, dim_c)
        }

    inl RL = 
        inl greedy_square sublayer =
            Layer.layer {
                layer_type = .action
                size = 1
                sublayer
                weights = const ()
                apply = inl _ -> Selector.greedy_square
                }

        inl greedy_square' action sublayer =
            Layer.layer {
                layer_type = .action
                size = 1
                sublayer
                weights = const ()
                apply = inl _ -> Selector.greedy_square' action
                }

        inl greedy_qr k dist_size sublayer =
            Layer.layer {
                layer_type = .action
                size = 1
                sublayer
                weights = const ()
                apply = inl _ x s -> 
                    inl f x = x.split (inl a,b -> a,(b/dist_size,dist_size))
                    Struct.map (function
                        | {primal adjoint block} as x -> {x with primal=f self; adjoint=f self}
                        | x -> f x
                        ) x
                    |> inl x -> Selector.greedy_qr k x s
                }

        inl greedy_kl {reward_range with from near_to} sublayer =
            Layer.layer {
                layer_type = .action
                size = 1
                sublayer
                weights = const ()
                apply = inl _ x s -> 
                    inl f x = x.split (inl a,b -> a, (b / HostTensor.span reward_range, reward_range))
                    Struct.map (function
                        | {primal adjoint block} as x -> {x with primal=f self; adjoint=f self}
                        | x -> f x
                        ) x
                    |> inl x -> Selector.greedy_kl x s
                }

        inl square_init {range state_type action_type} s =
            inl size = Struct.foldl (inl s x -> s + SerializerOneHot.span range x) 0
            inl state_size = size state_type
            inl action_size = size action_type

            input .input state_size
            //|> Feedforward.Layer.ln 0f32 256
            //|> Feedforward.Layer.relu 256
            |> Recurrent.Layer.mi 256
            |> Feedforward.Layer.linear action_size
            |> init s

        inl qr_init {distribution_size range state_type action_type} s =
            inl size = Struct.foldl (inl s x -> s + SerializerOneHot.span range x) 0
            inl state_size = size state_type
            inl action_size = size action_type * distribution_size

            input .input state_size
            //|> Feedforward.Layer.ln 0f32 256
            //|> Feedforward.Layer.relu 256
            |> Feedforward.Layer.linear action_size
            |> init s

        inl kl_init {reward_range range state_type action_type} s =
            inl size = Struct.foldl (inl s x -> s + SerializerOneHot.span range x) 0
            inl state_size = size state_type
            inl action_size = size action_type * HostTensor.span reward_range

            input .input state_size
            |> Recurrent.Layer.miln 0.05f32 256
            |> Recurrent.Layer.miln 0.05f32 256
            //|> Feedforward.Layer.ln 0f32 256
            //|> Feedforward.Layer.relu 256
            |> Feedforward.Layer.linear action_size
            |> init s

        /// For online learning.
        inl action {d with range state_type action_type net state} i s =
            indiv join
                assert (eq_type state_type i) "The input must be equal to the state type."
                inl one_hot_tensor l, size = s.CudaKernel.init {dim=1,size} (inl _ x -> Struct.foldl (inl s x' -> if x = x' then one else s) zero l)
                inl input = 
                    Struct.foldl_map (inl s x -> 
                        inl i, s' = SerializerOneHot.encode' range x
                        s + i, s + s'
                        ) 0 i
                    |> one_hot_tensor
                        
                inl a, {state bck} = 
                    match d with 
                    | {distribution_size} -> greedy_qr one distribution_size
                    | {reward_range} -> greedy_kl reward_range
                    | {action} -> SerializerOneHot.encode range action |> greedy_square'
                    | _ -> greedy_square
                    |> inl runner -> run (runner net) {state input={input}; bck=const()} s
                inl action = 
                    match a with
                    | a: int64 -> SerializerOneHot.decode range a action_type
                    | _ -> SerializerOneHot.decode range (s.CudaTensor.get (a 0)) action_type
                stack (action, {bck state})
        {square_init qr_init kl_init action}

