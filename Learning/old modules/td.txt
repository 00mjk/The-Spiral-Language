            // The TD layer uses the Zap update from the 'Fastest Convergence for Q-Learning' paper by Devray and Meyn 
            // in place of the standard PRONG update. It works better than standard TD, but is still a net negative in 
            // an AC agent.
            inl td (!default {w with !size}) = prong_template {front_mode=.zap; mode=.update} {w with size=1}

    // This player uses PG with a Zap TD(0) linear layer as the critic. Automatically adds the Zap layer 
    // to the critic and a linear layer for the actor. 
    inl player_td_ac {d with name learning_rate discount_factor !critic} cd =
        open Learning
        inl input_size = Union.length_dense State
        inl num_actions = Union.length_one_hot Action

        inl learning_rate = {
            actor=learning_rate
            critic=learning_rate ** 0.85f32
            shared=learning_rate
            }

        inl actor = match d with {actor} -> Tuple.append (Tuple.wrap actor) (RL.Layer.pg {size=num_actions}  :: ()) | _ -> RL.Layer.pg {size=num_actions}
        inl critic = RL.Layer.td {}
        inl shared = match d with {shared} -> shared | _ -> ()

        inl shared, shared_size = init cd input_size shared
        inl actor, _ = init cd shared_size actor
        inl critic, critic_size = init cd shared_size critic 

        inl block_critic_gradients =
            match d with
            | {block_critic_gradients} -> block_critic_gradients
            | _ -> true

        inl run = 
            Union.mutable_function 
                (inl {state={state with shared actor critic} input={input cd}} ->
                    assert (eq_type State input) "The input must be equal to the state type."
                    inl input = 
                        inl tns = Union.to_dense input |> HostTensor.array_as_tensor
                        cd.CudaTensor.from_host_tensor tns .reshape (inl x -> 1, Union.length_dense State)
                    inl shared, shared_out = run cd input shared
                    inl actor, actor_out = run cd shared_out actor
                    inl {out bck=actor_bck} = RL.sampling_pg actor_out cd
                    inl critic, critic_out = 
                        if block_critic_gradients then run cd (primal shared_out) critic
                        else run cd shared_out critic

                    inl bck {x with reward} = 
                        inl {value bck=critic_cost_bck} = 
                            inl value' = match x with {value'} -> value' | _ -> ()
                            RL.Value.td critic_out cd {discount_factor reward value'}
                        critic_cost_bck {learning_rate=learning_rate.critic}
                        match x with
                        | {state} -> critic.bck {learning_rate=learning_rate.critic; state discount_factor}
                        | _ -> critic.bck {learning_rate=learning_rate.critic}
                        actor_bck {discount_factor reward=value}
                        
                        Struct.foldr (inl {bck} _ -> bck {learning_rate=learning_rate.actor}) actor ()
                        Struct.foldr (inl {bck} _ -> bck {learning_rate=learning_rate.shared}) shared ()
                        {state=primal shared_out; value'=value}

                    inl action = Union.from_one_hot Action (cd.CudaTensor.get (out 0))
                    {state={actor critic shared bck}; out=action}
                    )
                {state={shared actor critic}; input={input=State; cd}}

        inl methods = {basic_methods with
            bet=inl s input -> s.data.run {input cd=s.data.cd}
            showdown=inl s reward -> 
                inl l = s.data.run.reset
                inl reward = dyn (to float32 reward)
                List.foldl' ignore (inl next x -> function 
                    | {bck} -> bck x |> inl x -> next {x with reward=0f32}
                    | _ -> next x
                    ) {reward} l

                inl cd = s.data.cd
                inl f learning_rate = Optimizer.standard learning_rate cd

                f learning_rate.shared s.data.shared
                f learning_rate.actor s.data.actor
                f learning_rate.critic s.data.critic
            game_over=inl s -> ()
            }

        Object
            .member_add methods
            .data_add {name; win=ref 0; shared actor critic run}
