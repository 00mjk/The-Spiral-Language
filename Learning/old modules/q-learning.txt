    /// The differentiable action selectors.
    inl Selector =
        inl reduce_actions x s = 
            s.CudaKernel.mapi_d1_redo_map {
                mapi_in=inl j i v _ -> v, i
                neutral_elem=-infinity,-1
                redo=inl a b -> if fst a > fst b then a else b
                } (primal x) ()
            |> HostTensor.unzip

        {
        greedy_square = inl prev x s ->
            inl v, a = reduce_actions x s
            (a, (v, a, adjoint x)), inl (reward: float64) ->
                match prev with
                | () -> ()
                | prev_v, prev_a, prev_x_a -> 
                    inl dim_a, dim_b = prev_x_a.dim
                    inl v, prev_v, prev_a = Tuple.map (s.CudaTensor.to_dev_tensor >> inl x j -> x j .get) (v, prev_v, prev_a)
                    inl prev_x_a = s.CudaTensor.to_dev_tensor prev_x_a
                    inl reward = to float reward
                    s.CudaKernel.iter {dim=dim_a} (inl j ->
                        inl span_a = HostTensor.span dim_a |> to float
                        inl v, prev_v, prev_a = Tuple.map (inl x -> x j) (v, prev_v, prev_a)
                        inl prev_x_a = prev_x_a j prev_a
                        prev_x_a.set (prev_x_a.get + square_bck (prev_v, v + reward) / span_a)
                        ) 

        greedy_qr = inl prev x s ->
            inl x_p = primal x
            inl a =
                s.CudaKernel.mapi_d1_dredo_map { 
                    redo_in = {
                        neutral_elem=0f32
                        redo=(+)
                        }
                    redo_mid = {
                        mapi_in=inl j i a -> a, i
                        neutral_elem=-infinityf32,-1
                        redo=inl a b -> if fst a > fst b then a else b
                        }
                    map_out = snd
                    } x_p

            (a, (a, x)), inl (reward: float64) ->
                match prev with
                | () -> ()
                | prev_a, {x with primal adjoint} -> 
                    inl dim_a, bim_b, dim_c = primal.dim
                    inl prev_a, a = Struct.map (s.CudaTensor.to_dev_tensor >> inl x j -> x j .get) (prev_a, a)
                    inl prev_x_p, prev_x_a, x_p = Struct.map s.CudaTensor.to_dev_tensor (primal, adjoint, x_p)
                    inl reward = to float reward
                    
                    // Note: Not optimized yet.
                    s.CudaKernel.iter {dim=dim_a, dim_c, dim_c} (inl k ->
                        inl span_a, span_c = Tuple.map (HostTensor.span >> to float) (dim_a, dim_c)
                        inl prev_a, prev_x_p, prev_x_a, x_p, a = Struct.map (inl x -> x k) (prev_a, prev_x_p, prev_x_a, x_p, a)
                        inl x_p = x_p a
                        inl prev_x_p, prev_x_a = Struct.map (inl x -> x prev_a) (prev_x_p, prev_x_a)
                        inl i ->
                        inl quantile = (to float i - to float 0.5) / span_c
                        inl x_p = x_p i .get
                        inl j ->
                        inl prev_x_p, prev_x_a = Struct.map (inl x -> x j) (prev_x_p, prev_x_a)
                        prev_x_a.set (prev_x_a.get + HQR.bck_a one quantile (prev_x_p.get, x_p + reward) / (span_c * span_a))
                        ) 
        }

    inl RL = 
        inl greedy_square sublayer =
            Layer.layer {
                layer_type = .action
                size = 1
                sublayer
                weights = const ()
                apply = inl _ -> Selector.greedy_square
                }

        inl greedy_qr dist_size sublayer =
            Layer.layer {
                layer_type = .action
                size = 1
                sublayer
                weights = const ()
                apply = inl _ prev x s -> 
                    inl f x = x.split (inl a,b -> a,(b/dist_size,dist_size))
                    Struct.map (function
                        | {primal adjoint block} as x -> {x with primal=f self; adjoint=f self}
                        | x -> f x
                        ) x
                    |> inl x -> Selector.greedy_qr prev x s
                }

        inl square_init {range state_type action_type} s =
            inl size = Struct.foldl (inl s x -> s + SerializerOneHot.span range x) 0
            inl state_size = size state_type
            inl action_size = size action_type

            input .input state_size
            //|> Feedforward.Layer.ln 0f32 256
            //|> Feedforward.Layer.tanh 256
            //|> Recurrent.Layer.mi 256
            |> Feedforward.Layer.linear action_size
            |> init s

        inl qr_init {distribution_size range state_type action_type} s =
            inl size = Struct.foldl (inl s x -> s + SerializerOneHot.span range x) 0
            inl state_size = size state_type
            inl action_size = size action_type * distribution_size

            input .input state_size
            //|> Feedforward.Layer.ln 0f32 256
            //|> Feedforward.Layer.relu 256
            |> Feedforward.Layer.linear action_size
            |> init s

        /// For online learning.
        inl action {d with range state_type action_type net state} i s =
            indiv join
                assert (eq_type state_type i) "The input must be equal to the state type."
                inl one_hot_tensor l, size = s.CudaKernel.init {dim=1,size} (inl _ x -> Struct.foldl (inl s x' -> if x = x' then one else s) zero l)
                inl input = 
                    Struct.foldl_map (inl s x -> 
                        inl i, s' = SerializerOneHot.encode' range x
                        s + i, s + s'
                        ) 0 i
                    |> one_hot_tensor
                        
                inl a, {state bck} = run net {state input={input}; bck=const()} s
                inl action = SerializerOneHot.decode range (s.CudaTensor.get (a 0)) action_type
                stack (action, {bck state})
        {square_init qr_init greedy_square greedy_qr action}